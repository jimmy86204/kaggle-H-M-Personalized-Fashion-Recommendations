{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cc6772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "from functools import reduce\n",
    "from lightgbm import LGBMRanker, early_stopping\n",
    "\n",
    "def create_customer_week_pair():\n",
    "    transactions = pd.read_parquet(f'./data/sample/transactions_train_sample_{sample}.parquet')\n",
    "    return transactions[(transactions.week >= start_week)].drop_duplicates([\"customer_id\", \"week\"])[[\"customer_id\", \"week\"]].reset_index(drop=True)\n",
    "\n",
    "def create_ground_truth():\n",
    "    transactions = pd.read_parquet(f'./data/sample/transactions_train_sample_{sample}.parquet')\n",
    "    s0 = transactions[(transactions.week >= start_week)].drop_duplicates([\"customer_id\", \"article_id\", \"week\"]).reset_index(drop=True)\n",
    "    s0[\"y\"] = 1\n",
    "    return s0[[\"customer_id\", \"article_id\", \"week\", \"y\"]]\n",
    "\n",
    "def create_top_strategy(weekly_customer):\n",
    "    transactions = pd.read_parquet(f'./data/sample/transactions_train_sample_{sample}.parquet')\n",
    "    customer = pd.read_parquet(f'./data/sample/customers_sample_{sample}.parquet')\n",
    "    customer.loc[customer.age == -1, \"age\"] = customer.age.mean()\n",
    "    customer_sex = pd.read_parquet('./data/fe/customer_sex.parquet')\n",
    "    top_history = pd.read_parquet('./data/fe/group_rank.parquet')\n",
    "    top_history.week += 1\n",
    "\n",
    "    transactions_with_gender = transactions \\\n",
    "        .merge(customer_sex, on=[\"customer_id\"], how=\"left\") \\\n",
    "        .merge(customer[[\"customer_id\", \"age\"]], on=[\"customer_id\"], how=\"left\")\n",
    "    transactions_with_gender[\"year\"] = 2022\n",
    "    transactions_with_gender[\"modify_year\"] = transactions_with_gender[\"t_dat\"].apply(lambda x: x.year)\n",
    "    transactions_with_gender[\"age\"] = transactions_with_gender[\"age\"] - (transactions_with_gender[\"year\"] - transactions_with_gender[\"modify_year\"])\n",
    "    bins = [0, 18, 25, 30, 35, 40, 45, 50, 55, 60, 999]\n",
    "    labels = [i for i in range(len(bins)-1)]\n",
    "    transactions_with_gender[\"age_group\"] = pd.cut(transactions_with_gender[\"age\"], bins=bins, labels=labels, right=False)\n",
    "    s1 = weekly_customer \\\n",
    "            .merge(transactions_with_gender[[\"customer_id\", \"gender\", \"age_group\", \"week\"]].drop_duplicates([\"customer_id\", \"week\"]), on=[\"customer_id\", \"week\"]) \\\n",
    "            .merge(top_history, on=[\"week\", \"gender\", \"age_group\"], how=\"inner\")\n",
    "    s1[\"top_strategy\"] = 1\n",
    "    return s1[[\"customer_id\", \"week\", \"top_strategy\", \"bestseller_rank\", \"article_id\"]]\n",
    "\n",
    "def create_duplicate_buy_strategy(weekly_customer):\n",
    "    transactions = pd.read_parquet(f'./data/sample/transactions_train_sample_{sample}.parquet')\n",
    "    output = pd.DataFrame()\n",
    "    for week in tqdm(range(start_week, 105)):\n",
    "        tmp = transactions[transactions.week < week]\n",
    "        tmp = tmp.drop_duplicates([\"customer_id\", \"article_id\", \"week\"])\n",
    "        last_week = tmp.groupby([\"customer_id\", \"article_id\"])[\"week\"].last().rename(\"previous_buy\").reset_index()\n",
    "        df = tmp.groupby([\"customer_id\", \"article_id\"]).size().rename(\"buy_times\").reset_index()\n",
    "        df = df[df.buy_times > 0].reset_index(drop=True)\n",
    "        df = df.merge(last_week, on=[\"customer_id\", \"article_id\"])\n",
    "        df[\"week\"] = week\n",
    "        output = pd.concat([output, df])\n",
    "    output[\"duplicate_buy_strategy\"] = 1\n",
    "    output[\"week_gap\"] = output[\"week\"] - output[\"previous_buy\"]\n",
    "    return weekly_customer.merge(output, on=[\"customer_id\", \"week\"], how=\"inner\")\n",
    "\n",
    "def create_recommendation_strategy(weekly_customer):\n",
    "    recommend_counts = 3\n",
    "    als = pd.read_parquet('./data/fe/als.parquet')\n",
    "    als = als[als.article_id != als.recommendation_article_id].reset_index(drop=True)\n",
    "    als = als.rename(columns={\"score\": \"item_score\"})\n",
    "    als_for_recommendation_candidate_generator = als.groupby([\"article_id\"]).head(recommend_counts).reset_index(drop=True)\n",
    "    transactions = pd.read_parquet(f'./data/sample/transactions_train_sample_{sample}.parquet')\n",
    "    output = pd.DataFrame()\n",
    "    for week in tqdm(range(start_week, 105)):\n",
    "        tmp = transactions[transactions.week < week]\n",
    "        tmp = tmp.drop_duplicates([\"customer_id\", \"article_id\", \"week\"])\n",
    "        duplicate_buy_df = tmp.groupby([\"customer_id\", \"article_id\"]).size().rename(\"buy_times\").reset_index()\n",
    "        duplicate_buy_df = duplicate_buy_df[duplicate_buy_df.buy_times > 0].reset_index(drop=True)\n",
    "        recommendation_candidate_df = pd.merge(duplicate_buy_df, als_for_recommendation_candidate_generator, \n",
    "                                               on=[\"article_id\"], how=\"left\")\n",
    "        recommendation_candidate_df = recommendation_candidate_df.drop(columns=[\"article_id\"]).rename(columns={\"recommendation_article_id\": \"article_id\"})\n",
    "        recommendation_candidate_df[\"recommendation_strategy\"] = recommendation_candidate_df.item_score\n",
    "        recommendation_candidate_df[\"week\"] = week\n",
    "        recommendation_candidate_df.drop_duplicates([\"customer_id\", \"article_id\"], inplace=True)\n",
    "        output = pd.concat([output, recommendation_candidate_df])\n",
    "        \n",
    "    return weekly_customer.merge(output[[\"customer_id\", \"article_id\", \"recommendation_strategy\", \"week\"]].reset_index(drop=True), on=[\"customer_id\", \"week\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "01280503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def apk(actual, predicted, k=10, perfect=False):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    if not perfect and len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    if perfect:\n",
    "        for i, a in enumerate(actual[:k]):\n",
    "            if a in predicted and a not in actual[:i]:\n",
    "                score += 1     \n",
    "    else:\n",
    "        for i,p in enumerate(predicted):\n",
    "            if p in actual and p not in predicted[:i]:\n",
    "                num_hits += 1.0\n",
    "                score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])\n",
    "\n",
    "# https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations/discussion/308635\n",
    "def customer_hex_id_to_int(series):\n",
    "    return series.str[-16:].apply(hex_id_to_int)\n",
    "\n",
    "def hex_id_to_int(str):\n",
    "    return int(str[-16:], 16)\n",
    "\n",
    "def article_id_str_to_int(series):\n",
    "    return series.astype('int32')\n",
    "\n",
    "def article_id_int_to_str(series):\n",
    "    return '0' + series.astype('str')\n",
    "\n",
    "class Categorize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_examples=0):\n",
    "        self.min_examples = min_examples\n",
    "        self.categories = []\n",
    "        \n",
    "    def fit(self, X):\n",
    "        for i in range(X.shape[1]):\n",
    "            vc = X.iloc[:, i].value_counts()\n",
    "            self.categories.append(vc[vc > self.min_examples].index.tolist())\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        data = {X.columns[i]: pd.Categorical(X.iloc[:, i], categories=self.categories[i]).codes for i in range(X.shape[1])}\n",
    "        return pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ab38fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 0.05\n",
    "start_week = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7ef3961d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f038ac8c79664d869c6e9eb547d09c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698eddcf7c5149f2aebc09d434d1edae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "customer_week_pair = create_customer_week_pair()\n",
    "ground_truth_stragegy = create_ground_truth()\n",
    "top_strategy = create_top_strategy(customer_week_pair)\n",
    "duplicate_buy_strategy = create_duplicate_buy_strategy(customer_week_pair)\n",
    "recommendation_strategy = create_recommendation_strategy(customer_week_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b6ee09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_merger = top_strategy \\\n",
    "                    .merge(duplicate_buy_strategy, on=[\"customer_id\", \"article_id\", \"week\"], how=\"outer\") \\\n",
    "                    .merge(recommendation_strategy, on=[\"customer_id\", \"article_id\", \"week\"], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "90492a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>week</th>\n",
       "      <th>top_strategy</th>\n",
       "      <th>bestseller_rank</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6219998949057481</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>827681002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6219998949057481</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>730683041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6219998949057481</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>877278003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6219998949057481</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>714790008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6219998949057481</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>759871002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10689142</th>\n",
       "      <td>13584458923080438626</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>917297004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10689143</th>\n",
       "      <td>13584458923080438626</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>918171001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10689144</th>\n",
       "      <td>13584458923080438626</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>919978002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10689145</th>\n",
       "      <td>13584458923080438626</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>920610002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10689146</th>\n",
       "      <td>13584458923080438626</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>930533003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10689147 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   customer_id  week  top_strategy  bestseller_rank  \\\n",
       "0             6219998949057481    80             1                1   \n",
       "1             6219998949057481    80             1                2   \n",
       "2             6219998949057481    80             1                3   \n",
       "3             6219998949057481    80             1                4   \n",
       "4             6219998949057481    80             1                5   \n",
       "...                        ...   ...           ...              ...   \n",
       "10689142  13584458923080438626   104             1               17   \n",
       "10689143  13584458923080438626   104             1               17   \n",
       "10689144  13584458923080438626   104             1               17   \n",
       "10689145  13584458923080438626   104             1               17   \n",
       "10689146  13584458923080438626   104             1               17   \n",
       "\n",
       "          article_id  \n",
       "0          827681002  \n",
       "1          730683041  \n",
       "2          877278003  \n",
       "3          714790008  \n",
       "4          759871002  \n",
       "...              ...  \n",
       "10689142   917297004  \n",
       "10689143   918171001  \n",
       "10689144   919978002  \n",
       "10689145   920610002  \n",
       "10689146   930533003  \n",
       "\n",
       "[10689147 rows x 5 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "704b864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = candidate_merger.merge(ground_truth_stragegy, on=[\"customer_id\", \"article_id\", \"week\"], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "22a90c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"y\"].fillna(0, inplace=True)\n",
    "df[\"top_strategy\"].fillna(0, inplace=True)\n",
    "df[\"bestseller_rank\"].fillna(999, inplace=True)\n",
    "df[\"buy_times\"].fillna(0, inplace=True)\n",
    "df[\"duplicate_buy_strategy\"].fillna(0, inplace=True)\n",
    "df[\"week_gap\"].fillna(999, inplace=True)\n",
    "df[\"recommendation_strategy\"].fillna(0, inplace=True)\n",
    "del df[\"previous_buy\"]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8e27f10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del customer_week_pair, ground_truth_stragegy, top_strategy, duplicate_buy_strategy, recommendation_strategy, candidate_merger\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "65109bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279.43123724475413"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"customer_id\", \"week\"]).article_id.count().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "82862991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013013388451848457"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4ca94ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### read meta data ###\n",
    "static_article_df = pd.read_parquet('./data/fe/static_article_df.parquet')[[\"article_id\", \"duplicate_hold_rate\"]]\n",
    "df = df.merge(static_article_df, on=[\"article_id\"])\n",
    "del static_article_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "41b04c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_article_df = pd.read_parquet('./data/fe/dynamic_article_df.parquet')\n",
    "dynamic_article_df.week += 1\n",
    "df = df.merge(dynamic_article_df, on=[\"article_id\", \"week\"], how=\"left\")\n",
    "del dynamic_article_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "df11d9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_features = pd.read_parquet('./data/fe/customer_features.parquet')[[\"customer_id\", \"gender\", \"dup_rate\"]]\n",
    "df = df.merge(customer_features, on=[\"customer_id\"])\n",
    "del customer_features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f59aed6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-862c61fabce2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcustomer_article_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/fe/customer_recommendation_fully.parquet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"recommendation_article_id\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"article_id\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustomer_article_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"customer_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"article_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mcustomer_article_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "customer_article_score = pd.read_parquet('./data/fe/customer_recommendation_fully.parquet').rename(columns={\"recommendation_article_id\": \"article_id\"})\n",
    "df = df.merge(customer_article_score, on=[\"customer_id\", \"article_id\"], how=\"left\")\n",
    "del customer_article_score\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "957399ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4245900472157</td>\n",
       "      <td>715624001</td>\n",
       "      <td>0.090816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4245900472157</td>\n",
       "      <td>715624010</td>\n",
       "      <td>0.057217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4245900472157</td>\n",
       "      <td>189616006</td>\n",
       "      <td>0.050851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4245900472157</td>\n",
       "      <td>547780001</td>\n",
       "      <td>0.046596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4245900472157</td>\n",
       "      <td>803757001</td>\n",
       "      <td>0.041995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21796491</th>\n",
       "      <td>18446737527580148316</td>\n",
       "      <td>800691008</td>\n",
       "      <td>0.120424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21796492</th>\n",
       "      <td>18446737527580148316</td>\n",
       "      <td>629420007</td>\n",
       "      <td>0.104970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21796493</th>\n",
       "      <td>18446737527580148316</td>\n",
       "      <td>803757004</td>\n",
       "      <td>0.100142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21796494</th>\n",
       "      <td>18446737527580148316</td>\n",
       "      <td>715624001</td>\n",
       "      <td>0.093450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21796495</th>\n",
       "      <td>18446737527580148316</td>\n",
       "      <td>760084003</td>\n",
       "      <td>0.092234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21796496 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   customer_id  article_id     score\n",
       "0                4245900472157   715624001  0.090816\n",
       "1                4245900472157   715624010  0.057217\n",
       "2                4245900472157   189616006  0.050851\n",
       "3                4245900472157   547780001  0.046596\n",
       "4                4245900472157   803757001  0.041995\n",
       "...                        ...         ...       ...\n",
       "21796491  18446737527580148316   800691008  0.120424\n",
       "21796492  18446737527580148316   629420007  0.104970\n",
       "21796493  18446737527580148316   803757004  0.100142\n",
       "21796494  18446737527580148316   715624001  0.093450\n",
       "21796495  18446737527580148316   760084003  0.092234\n",
       "\n",
       "[21796496 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_article_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "30ca7130",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.score.fillna(0, inplace=True)\n",
    "df[\"rank\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "84749cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d266a1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "74d9307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_use = [\n",
    "       'week', 'top_strategy',\n",
    "       'bestseller_rank', \n",
    "       'buy_times', 'duplicate_buy_strategy', 'week_gap',\n",
    "       'recommendation_strategy', \n",
    "       'duplicate_hold_rate', 'price',\n",
    "       'weekly_sell_counts', 'rank', 'gender', 'dup_rate', 'score'\n",
    "]\n",
    "target = \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "18fee11b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.896237\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.464629\n",
      "[LightGBM] [Debug] init for col-wise cost 0.245181 seconds, init for row-wise cost 1.036074 seconds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.364803 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Sparse Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 2030\n",
      "[LightGBM] [Info] Number of data points in the train set: 23817856, number of used features: 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 8\n",
      "[1]\tvalid_0's ndcg@1: 0.719531\tvalid_0's ndcg@2: 0.720836\tvalid_0's ndcg@3: 0.723031\tvalid_0's ndcg@4: 0.727182\tvalid_0's ndcg@5: 0.730065\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[2]\tvalid_0's ndcg@1: 0.725546\tvalid_0's ndcg@2: 0.722643\tvalid_0's ndcg@3: 0.725618\tvalid_0's ndcg@4: 0.72958\tvalid_0's ndcg@5: 0.732482\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[3]\tvalid_0's ndcg@1: 0.725546\tvalid_0's ndcg@2: 0.723442\tvalid_0's ndcg@3: 0.725968\tvalid_0's ndcg@4: 0.729704\tvalid_0's ndcg@5: 0.732414\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 8\n",
      "[4]\tvalid_0's ndcg@1: 0.726179\tvalid_0's ndcg@2: 0.723966\tvalid_0's ndcg@3: 0.727252\tvalid_0's ndcg@4: 0.730515\tvalid_0's ndcg@5: 0.733041\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 8\n",
      "[5]\tvalid_0's ndcg@1: 0.726179\tvalid_0's ndcg@2: 0.724075\tvalid_0's ndcg@3: 0.727148\tvalid_0's ndcg@4: 0.730595\tvalid_0's ndcg@5: 0.733137\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 8\n",
      "[6]\tvalid_0's ndcg@1: 0.725546\tvalid_0's ndcg@2: 0.724377\tvalid_0's ndcg@3: 0.727655\tvalid_0's ndcg@4: 0.730495\tvalid_0's ndcg@5: 0.733178\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 8\n",
      "[7]\tvalid_0's ndcg@1: 0.72523\tvalid_0's ndcg@2: 0.723893\tvalid_0's ndcg@3: 0.727595\tvalid_0's ndcg@4: 0.73051\tvalid_0's ndcg@5: 0.732738\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 8\n",
      "[8]\tvalid_0's ndcg@1: 0.72523\tvalid_0's ndcg@2: 0.723693\tvalid_0's ndcg@3: 0.727334\tvalid_0's ndcg@4: 0.730237\tvalid_0's ndcg@5: 0.732721\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 8\n",
      "[9]\tvalid_0's ndcg@1: 0.725546\tvalid_0's ndcg@2: 0.72381\tvalid_0's ndcg@3: 0.727438\tvalid_0's ndcg@4: 0.73051\tvalid_0's ndcg@5: 0.733079\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 8\n",
      "[10]\tvalid_0's ndcg@1: 0.724913\tvalid_0's ndcg@2: 0.724201\tvalid_0's ndcg@3: 0.727529\tvalid_0's ndcg@4: 0.731005\tvalid_0's ndcg@5: 0.733202\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 8\n",
      "[11]\tvalid_0's ndcg@1: 0.726179\tvalid_0's ndcg@2: 0.724288\tvalid_0's ndcg@3: 0.727553\tvalid_0's ndcg@4: 0.730976\tvalid_0's ndcg@5: 0.73376\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 8\n",
      "[12]\tvalid_0's ndcg@1: 0.724913\tvalid_0's ndcg@2: 0.724079\tvalid_0's ndcg@3: 0.727016\tvalid_0's ndcg@4: 0.73086\tvalid_0's ndcg@5: 0.733559\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 8\n",
      "[13]\tvalid_0's ndcg@1: 0.726496\tvalid_0's ndcg@2: 0.724205\tvalid_0's ndcg@3: 0.728026\tvalid_0's ndcg@4: 0.731113\tvalid_0's ndcg@5: 0.734356\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 8\n",
      "[14]\tvalid_0's ndcg@1: 0.725546\tvalid_0's ndcg@2: 0.723887\tvalid_0's ndcg@3: 0.728127\tvalid_0's ndcg@4: 0.73109\tvalid_0's ndcg@5: 0.734443\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 9\n",
      "[15]\tvalid_0's ndcg@1: 0.726179\tvalid_0's ndcg@2: 0.724952\tvalid_0's ndcg@3: 0.728425\tvalid_0's ndcg@4: 0.731253\tvalid_0's ndcg@5: 0.735093\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 9\n",
      "[16]\tvalid_0's ndcg@1: 0.726179\tvalid_0's ndcg@2: 0.725029\tvalid_0's ndcg@3: 0.728567\tvalid_0's ndcg@4: 0.731493\tvalid_0's ndcg@5: 0.73526\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 8\n",
      "[17]\tvalid_0's ndcg@1: 0.726179\tvalid_0's ndcg@2: 0.725229\tvalid_0's ndcg@3: 0.728489\tvalid_0's ndcg@4: 0.731015\tvalid_0's ndcg@5: 0.734552\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 8\n",
      "[18]\tvalid_0's ndcg@1: 0.726179\tvalid_0's ndcg@2: 0.724874\tvalid_0's ndcg@3: 0.728355\tvalid_0's ndcg@4: 0.731391\tvalid_0's ndcg@5: 0.734886\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 9\n",
      "[19]\tvalid_0's ndcg@1: 0.726812\tvalid_0's ndcg@2: 0.725172\tvalid_0's ndcg@3: 0.728225\tvalid_0's ndcg@4: 0.731607\tvalid_0's ndcg@5: 0.734702\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 9\n",
      "[20]\tvalid_0's ndcg@1: 0.725546\tvalid_0's ndcg@2: 0.72484\tvalid_0's ndcg@3: 0.728134\tvalid_0's ndcg@4: 0.731356\tvalid_0's ndcg@5: 0.734481\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 8\n",
      "[21]\tvalid_0's ndcg@1: 0.726496\tvalid_0's ndcg@2: 0.725023\tvalid_0's ndcg@3: 0.728148\tvalid_0's ndcg@4: 0.731729\tvalid_0's ndcg@5: 0.734734\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 9\n",
      "[22]\tvalid_0's ndcg@1: 0.725546\tvalid_0's ndcg@2: 0.72582\tvalid_0's ndcg@3: 0.72904\tvalid_0's ndcg@4: 0.732345\tvalid_0's ndcg@5: 0.735215\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 9\n",
      "[23]\tvalid_0's ndcg@1: 0.725546\tvalid_0's ndcg@2: 0.72582\tvalid_0's ndcg@3: 0.728943\tvalid_0's ndcg@4: 0.732446\tvalid_0's ndcg@5: 0.735458\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 8\n",
      "[24]\tvalid_0's ndcg@1: 0.725863\tvalid_0's ndcg@2: 0.725524\tvalid_0's ndcg@3: 0.728701\tvalid_0's ndcg@4: 0.732264\tvalid_0's ndcg@5: 0.735545\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 9\n",
      "[25]\tvalid_0's ndcg@1: 0.726179\tvalid_0's ndcg@2: 0.725151\tvalid_0's ndcg@3: 0.728789\tvalid_0's ndcg@4: 0.732111\tvalid_0's ndcg@5: 0.735554\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 9\n",
      "[26]\tvalid_0's ndcg@1: 0.727445\tvalid_0's ndcg@2: 0.725406\tvalid_0's ndcg@3: 0.728716\tvalid_0's ndcg@4: 0.732336\tvalid_0's ndcg@5: 0.735389\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 8\n",
      "[27]\tvalid_0's ndcg@1: 0.726812\tvalid_0's ndcg@2: 0.725507\tvalid_0's ndcg@3: 0.729149\tvalid_0's ndcg@4: 0.732399\tvalid_0's ndcg@5: 0.73581\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 9\n",
      "[28]\tvalid_0's ndcg@1: 0.726812\tvalid_0's ndcg@2: 0.725385\tvalid_0's ndcg@3: 0.728966\tvalid_0's ndcg@4: 0.732299\tvalid_0's ndcg@5: 0.735785\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 8\n",
      "[29]\tvalid_0's ndcg@1: 0.726496\tvalid_0's ndcg@2: 0.725268\tvalid_0's ndcg@3: 0.728471\tvalid_0's ndcg@4: 0.732253\tvalid_0's ndcg@5: 0.735684\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 9\n",
      "[30]\tvalid_0's ndcg@1: 0.727129\tvalid_0's ndcg@2: 0.725914\tvalid_0's ndcg@3: 0.728658\tvalid_0's ndcg@4: 0.732415\tvalid_0's ndcg@5: 0.735801\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 9\n",
      "[31]\tvalid_0's ndcg@1: 0.727445\tvalid_0's ndcg@2: 0.725786\tvalid_0's ndcg@3: 0.728672\tvalid_0's ndcg@4: 0.732556\tvalid_0's ndcg@5: 0.735748\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 9\n",
      "[32]\tvalid_0's ndcg@1: 0.726179\tvalid_0's ndcg@2: 0.725532\tvalid_0's ndcg@3: 0.728592\tvalid_0's ndcg@4: 0.732297\tvalid_0's ndcg@5: 0.735512\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 9\n",
      "[33]\tvalid_0's ndcg@1: 0.726496\tvalid_0's ndcg@2: 0.725603\tvalid_0's ndcg@3: 0.728286\tvalid_0's ndcg@4: 0.732619\tvalid_0's ndcg@5: 0.735637\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 10\n",
      "[34]\tvalid_0's ndcg@1: 0.726812\tvalid_0's ndcg@2: 0.725553\tvalid_0's ndcg@3: 0.728174\tvalid_0's ndcg@4: 0.732791\tvalid_0's ndcg@5: 0.735725\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 8\n",
      "[35]\tvalid_0's ndcg@1: 0.727129\tvalid_0's ndcg@2: 0.725669\tvalid_0's ndcg@3: 0.728072\tvalid_0's ndcg@4: 0.732741\tvalid_0's ndcg@5: 0.735552\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 8\n",
      "[36]\tvalid_0's ndcg@1: 0.726812\tvalid_0's ndcg@2: 0.725553\tvalid_0's ndcg@3: 0.727858\tvalid_0's ndcg@4: 0.732546\tvalid_0's ndcg@5: 0.735416\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 9\n",
      "[37]\tvalid_0's ndcg@1: 0.727445\tvalid_0's ndcg@2: 0.725619\tvalid_0's ndcg@3: 0.72822\tvalid_0's ndcg@4: 0.732594\tvalid_0's ndcg@5: 0.735643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 9\n",
      "[38]\tvalid_0's ndcg@1: 0.727445\tvalid_0's ndcg@2: 0.725741\tvalid_0's ndcg@3: 0.7285\tvalid_0's ndcg@4: 0.732958\tvalid_0's ndcg@5: 0.7358\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 10\n",
      "[39]\tvalid_0's ndcg@1: 0.727129\tvalid_0's ndcg@2: 0.726391\tvalid_0's ndcg@3: 0.728551\tvalid_0's ndcg@4: 0.733271\tvalid_0's ndcg@5: 0.736061\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 10\n",
      "[40]\tvalid_0's ndcg@1: 0.727129\tvalid_0's ndcg@2: 0.726191\tvalid_0's ndcg@3: 0.728729\tvalid_0's ndcg@4: 0.733313\tvalid_0's ndcg@5: 0.736207\n"
     ]
    }
   ],
   "source": [
    "n_folds = 1\n",
    "last_week = 104\n",
    "params = {\n",
    "    \"objective\":\"lambdarank\",\n",
    "    \"metric\":\"ndcg\",\n",
    "    \"boosting_type\":\"dart\",\n",
    "    \"n_estimators\":40,\n",
    "    \"importance_type\":'gain',\n",
    "    \"verbose\":10,\n",
    "    \"learning_rate\":0.1,\n",
    "    \"num_leaves\": 40,\n",
    "}\n",
    "\n",
    "\n",
    "for i in range(n_folds):\n",
    "    spilt_week = last_week - (n_folds - i) + 1\n",
    "    training_data = df[df.week < spilt_week]\n",
    "    valid_data    = df[(df.week == spilt_week) & ~((df.top_strategy == 0) & (df.duplicate_buy_strategy == 0) & (df.recommendation_strategy == 0))]\n",
    "    train_baskets = training_data.groupby(['week', 'customer_id'])['article_id'].count().values\n",
    "    validation_baskets = valid_data.groupby(['week', 'customer_id'])['article_id'].count().values\n",
    "    train_X, train_y = training_data[columns_to_use], training_data[target]\n",
    "    val_X, val_y     = valid_data[columns_to_use], valid_data[target]\n",
    "    ranker = LGBMRanker(\n",
    "        **params,\n",
    "    )\n",
    "    ranker = ranker.fit(\n",
    "        train_X,\n",
    "        train_y,\n",
    "        group=train_baskets,\n",
    "        eval_set=[(val_X, val_y)],\n",
    "        eval_group=[validation_baskets],\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e9d6ea3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommendation_strategy 0.4345256053843216\n",
      "bestseller_rank 0.18644609741145515\n",
      "buy_times 0.176881165270071\n",
      "top_strategy 0.09924206324997073\n",
      "week_gap 0.04608174861917835\n",
      "score 0.034814855179167464\n",
      "weekly_sell_counts 0.012046679363147645\n",
      "rank 0.0062038566789821385\n",
      "duplicate_hold_rate 0.001291180222235345\n",
      "week 0.0012354380432170722\n",
      "price 0.0010113328982032338\n",
      "dup_rate 0.0002014427495379371\n",
      "gender 1.8534930512341454e-05\n",
      "duplicate_buy_strategy 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in ranker.feature_importances_.argsort()[::-1]:\n",
    "    print(columns_to_use[i], ranker.feature_importances_[i]/ranker.feature_importances_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "221cd708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-93-2ea9d50287a9>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_data[\"preds\"] = preds\n"
     ]
    }
   ],
   "source": [
    "preds = ranker.predict(val_X)\n",
    "valid_data[\"preds\"] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "92e54ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_id2predicted_article_ids = valid_data[[\"customer_id\", \"article_id\", \"preds\"]] \\\n",
    "    .sort_values(['customer_id', 'preds'], ascending=False) \\\n",
    "    .groupby('customer_id')['article_id'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0265fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = valid_data[[\"customer_id\", \"article_id\", \"y\"]]\n",
    "tmp = tmp[tmp.y == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ef055096",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_parquet('./data/sample/transactions_train_sample_0.05.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a381a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_week_purchases_by_cust = tmp[tmp.week == spilt_week].groupby('customer_id').article_id.apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f8defaa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04789038823871275"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apks = []\n",
    "cold = 0\n",
    "for c_id, gt in test_week_purchases_by_cust.items():\n",
    "    temp = []\n",
    "    if c_id in c_id2predicted_article_ids:\n",
    "        temp = c_id2predicted_article_ids[c_id]\n",
    "    else:\n",
    "        cold += 1\n",
    "    apks.append(apk(gt, temp, 12, False))\n",
    "np.mean(apks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5d8ff1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0356058206237468"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apks = []\n",
    "cold = 0\n",
    "for c_id, gt in test_week_purchases_by_cust.items():\n",
    "    temp = []\n",
    "    if c_id in c_id2predicted_article_ids:\n",
    "        temp = c_id2predicted_article_ids[c_id]\n",
    "    else:\n",
    "        cold += 1\n",
    "    apks.append(apk(gt, temp, 12, False))\n",
    "np.mean(apks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70b97e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03398928476970805"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apks = []\n",
    "cold = 0\n",
    "for c_id, gt in test_week_purchases_by_cust.items():\n",
    "    temp = []\n",
    "    if c_id in c_id2predicted_article_ids:\n",
    "        temp = c_id2predicted_article_ids[c_id]\n",
    "    else:\n",
    "        cold += 1\n",
    "    apks.append(apk(gt, temp, 12, False))\n",
    "np.mean(apks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c90992a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3283"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c_id2predicted_article_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2954cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
